\section{Introduction}

The game \emph{2048} is a stochastic single-player game developed by G. Cirulli~\cite{2048}.
Several computer players have been implemented, among which the most successful approaches employ N-tuple networks as evaluation functions in combination with Expectimax search.
For instance, the state-of-the-art player developed by Guei et al.~\cite{Guei22} achieved an average score of 625\,377 by using two-stage N-tuple networks consisting of 8 $\times$ 6-tuples---trained by extended temporal-difference learning with optimistic initialization---, 6-ply expectimax search, and a game-specific tile-downgrading technique.


In general, larger N-tuple networks yield better performance at the cost of longer training times~\cite{Terauchi25}.
Let N-tuple networks consist of $m$ $\times$ $n$-tuples, then we can consider two approaches to enlarging N-tuple networks.
The first approach is to increase the number $m$ of tuples.  While this is relatiely straightforward to implement, Oka and Matsuzaki~\cite{OkMa16} demonstrated that the performance saturated only with this approach.
The second approach is to increase the tuple size $n$.  In the early stages of research on 2048, computer players improved the performance by enlarging 4-tuples to 6-tuples.  However, as Jaskowski~\cite{Jask17} suggested, further enlargement has been considered infeasible for the game 2048 due to the exponential growth in the number of parameters.  Note that, in 2048, a board cell can take 18 possible values, and N-tuple networks with $m$ $\times$ $n$-tuples have $m\times 18^n$ parameters.  A 6-tuple network has $3.4\times 10^7$ parameters (13 GB of memory~\footnote{The memory size is calculated for 2-stage 8 $\times$ $n$-tuple with temporal coherence learning~\cite{Jask17} assuming 64 bits per weight.}), a 7-tuple network has $6.1\times 10^8$ parameters (235 GB), and an 8-tuple has $1.1\times 10^10$ parameters (4.2 TB).


In this study, we overcome the aforementioned limitation by proposing a novel encoding method, named \emph{Vertical Split Encoding} (\emph{VSE}). In $k$-VSE, the 18 possibe cell values are divided into $k$ value ranges. For each value range, a board state is encoded while ignoring values outside the range (except for empty cells, represented as 0).
For example, in 2-VSE, the 18 possible values are divided into two value ranges [0--9] and [10--18]. A state [0, 1, 11, 12] is then encoded as $[0, 1, L, L]$ and $[0, S, 11, 12]$, where $L$ and $S$ are special labels denoting \emph{larger} and \emph{smaller}, respectively.
With this method, the numbers of parameters required for $m$ $\times$ $n$-tuples are reduced to $m\times 11^n$ with 2-VSE, $m\times 9^n$ with 3-VSE, and $m\times 7^n$ with 4-VSE. The reduction is drastic: for instance when $n=8$, the numbers of parameters is reduced to 3.9\% with 2-VSE, 1.2\% with 3-VSE, and 0.21\% with 4-VSE.


We demonstrate the effectiveness of VSE in the context of 2048.
We developed several computer players using N-tuple networks: 6-tuples without VSE, 7-tuples with 2-VSE, 8-tuples with 3-VSE, and 9-tuples with 4-VSE.  All the N-tuple networks are successfully trained using at most 64 GB of memory.
The 7-, 8-, and 9-tuple networks outperformed the 6-tuple baseline.
In particular, the best 8-tuple networks achieved an average score of 407\,067 with greedy play, 549\,574 with 3-ply Expectimax, and 594\,619 with 5-ply Expectimax.
